<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-13T11:35:58+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Roshan homepage</title><author><name>rsatapat</name><email>rsatapat@yahoo.com</email></author><entry><title type="html">On the many faces of matrix multiplication</title><link href="http://localhost:4000/blogs/matrix-multiplication/" rel="alternate" type="text/html" title="On the many faces of matrix multiplication" /><published>2025-07-02T00:00:00+02:00</published><updated>2025-07-02T00:00:00+02:00</updated><id>http://localhost:4000/blogs/matrix-multiplication</id><content type="html" xml:base="http://localhost:4000/blogs/matrix-multiplication/"><![CDATA[<p>It’s no exaggeration to say that our world runs on linear algebra. In fact, much of machine learning, at least the parts used most often, is essentially just a series of matrix multiplications.<br />
If you hold these assertions to be true, then understanding matrix multiplication is essential for understanding ML, data analysis, and the world (okay, that last one might be a bit of a stretch). And yet, schools teach matrix multiplication in the worst way imaginable. It is introduced without any context or reference to its usefulness. Furthermore, the algorithm that is taught to perform matrix multiplication seems very arbitrary and unmotivated. I remember feeling absolutely bewildered by it and, being a lazy bastard, felt no inclination to remember it. Imagine my shock when I realized how intuitive and ubiquitous matrix multiplication is.</p>

<p>First, we have to think of matrices in terms of rows and columns. This may sound obvious, but it’s worth remembering that the numbers inside a matrix have no meaning except as part of a row or column.<br />
Matrix multiplication A×B is only defined if A has dimensions (m×n) and B has dimensions (n×l). In other words, the number of columns in A must equal the number of rows in B. In this case, the product matrix will have dimensions (m×l).</p>

<p>With that in mind, there are 4 ways to think of matrix multiplication:</p>
<ol>
  <li>Row with column: Populate element by element</li>
  <li>Columns with row: Sum of many component matrices</li>
  <li>Columns with column: Populate column by column, scale, and then add columns</li>
  <li>Row with row: Populate row by row, scale, and then add rows</li>
</ol>

<p>Of course, all these interpretations of matrix multiplication are mathematically equivalent. The value in knowing them is that you can choose the perspective that best fits the situation, making that particular mathematical operation clearer and more intuitive.</p>

<p>All the videos in this post were created using the <a href="https://www.manim.community/">Manim library</a>, and the code can be found <a href="https://github.com/rsatapat/matrix_multiplication_viz">here</a>.</p>

<h3 id="row-column-multiplication">Row-Column multiplication</h3>

<p>In this view, the product matrix is filled element by element. The element at position (i, j) is the dot product (or inner product) of the i-th row of matrix A and the j-th column of matrix B. The inner product is a foundational concept in linear algebra: it is large when the two vectors point in roughly the same direction in space, and small or negative when they point in different directions. The inner product between two vectors can also be viewed as a measure of their covariance. This is why the row–column perspective of matrix multiplication connects naturally to computing a <a href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a>, which is obtained by multiplying a matrix by its transpose: A × Aᵀ.</p>

<div class="row mt-3 align-items-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  
    <video src="/assets/video/MatrixMultiplicationDemo_rowcol.mp4" class="img-fluid rounded" width="100%" height="auto" title="example image" autoplay="" controls="">
    </video>  

  
  
</figure>

    </div>
</div>

<h3 id="column-row-multiplication">Column-Row multiplication</h3>

<p>In this view, the product matrix is seen as the sum of several component matrices. Each component matrix is formed by taking the outer product of the i-th column of matrix A with the i-th row of matrix B. Only columns and rows with the same index are paired in this way.<br />
This column–row perspective makes the idea behind Principal Component Analysis (PCA) much easier to grasp. In PCA, the goal is to reconstruct the original data as the product of two matrices, A and B, where the number of columns in A is far smaller than the original dimensionality of the data. So, if you decide to keep 2 components, you are effectively approximating your data as the sum of two matrices.</p>

<div class="row mt-3 align-items-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  
    <video src="/assets/video/MatrixMultiplicationDemo_colrow.mp4" class="img-fluid rounded" width="100%" height="auto" title="example image" autoplay="" controls="">
    </video>  

  
  
</figure>

    </div>
</div>

<h3 id="column-column-multiplication">Column-Column multiplication</h3>

<p>In this view, the product matrix is populated column by column. Each column of the product matrix is formed by taking a weighted sum of the columns of matrix A. For the i-th column of the product, take each column j of B, multiply it by the element B(i, j), and then add up all these scaled columns. This view is useful when your original data is in matrix A. In that case, matrix B represents an operation applied to A that scales and combines its columns.</p>

<div class="row mt-3 align-items-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  
    <video src="/assets/video/MatrixMultiplicationDemo_colcol.mp4" class="img-fluid rounded" width="100%" height="auto" title="example image" autoplay="" controls="">
    </video>  

  
  
</figure>

    </div>
</div>

<h3 id="row-row-multiplication">Row-Row multiplication</h3>

<p>In this view, the product matrix is populated row by row. Each row of the product matrix is formed by taking a weighted sum of the rows of matrix B. For the i-th row of the product, take each row j of B, multiply it by the element A(i, j), and then add up all these scaled rows. This view is useful when your original data is in matrix B. In that case, matrix A represents an operation applied to B that scales and combines its rows.</p>

<div class="row mt-3 align-items-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  
    <video src="/assets/video/MatrixMultiplicationDemo_rowrow.mp4" class="img-fluid rounded" width="100%" height="auto" title="example image" autoplay="" controls="">
    </video>  

  
  
</figure>

    </div>
</div>]]></content><author><name>rsatapat</name><email>rsatapat@yahoo.com</email></author><category term="Math" /><category term="Data-analysis" /><summary type="html"><![CDATA[It’s no exaggeration to say that our world runs on linear algebra. In fact, much of machine learning, at least the parts used most often, is essentially just a series of matrix multiplications.]]></summary></entry><entry><title type="html">On the messiness of data</title><link href="http://localhost:4000/blogs/messy-data/" rel="alternate" type="text/html" title="On the messiness of data" /><published>2025-06-03T00:00:00+02:00</published><updated>2025-06-03T00:00:00+02:00</updated><id>http://localhost:4000/blogs/messy-data</id><content type="html" xml:base="http://localhost:4000/blogs/messy-data/"><![CDATA[<p>For a data analyst, a large datasets can be a double-edge sword. When used properly, they can yield powerful insights and leave you with a deeper understaning of the world around you. However, in the absence of good data organisation skills, you are only left with wasted effort and confusion. I encountered this challenge firsthand during my PhD.</p>

<p>It took me many years to get a clear, high-level picture of data. If I had this picture when I started my PhD, I would have made different choices and could have saved siginificant amount of time. Some of the lessons that I learned through trial-and-error might be obvious to people who have been trained in data analysis or computer sciences or have been involed in data-related projects in the past. But, if you are a novice having to deal with large amounts of data, please read on.</p>

<p>Before going any further, let me briefly explain the setting. I performed experiments with fruitfly that involved introducing a genetic perturbation in the fly and then, studing the effects of the perturbation on its behaviour, specifically, its ability to walk.</p>

<p>I categorise the data into three types:</p>
<ol>
  <li>Raw data: The output of the experimental system.</li>
  <li>Pre-processed data: The data that has undergone some preliminary cleaning to get rid of anomalies and idiosyncracies</li>
  <li>Processed data: The final form of the data that is used for scientific analysis.</li>
</ol>

<h3 id="combining-data">Combining data</h3>
<p>Imagine you have position (x, y) data for 10 animals, collected over 5 sessions per animal, with 5 stimulus conditions per session. There are many ways to organize this data. The simplest approach is to store each dataset in a separate file. This would result in:<br />
2 (x and y) × 10 (animals) × 5 (sessions) × 5 (stimuli) = 500 files.<br />
One might argue that it’s unnecessary to store x and y separately. Combining them would reduce the number of files to 250. Taking this thought to its logical conclusion, we could consolidate all the data into a single file. This would be ideal: fewer files to track, lower risk of losing or misplacing data, and a cleaner, more centralized dataset.<br />
However, doing so requires representing the data as a 4D structure: position × animal × session × stimulus. Is that possible?</p>

<h3 id="data-retrieval">Data retrieval</h3>
<p>The main purpose of storing data if of course, to be able to retrieve it when needed. And in order to retrieve data, there has to be a way to identify it. 
There are two ways in data is indentified and retrived from a dataset:</p>
<ul>
  <li>By filtering: Use some logic to reject/whittle down data till only your desired data remains</li>
  <li>By indexing: Use an index that uniquely identifies each unit of data in the dataset<br />
Ideally, we want a data format that makes retrieval simple and intuitive, so that anyone with access can slice and dice the dataset in whatever way they need, without requiring much additional context. The structure should be self-evident and easy to navigate.</li>
</ul>

<h3 id="metadata-and-processing">Metadata and processing</h3>
<p>In the context of behavioural experiments that I was doing, the data was the x,y position and the head direction of the fly. The metadata was fly identity, type of genetic perturbation, and the features of the visual stimulus.<br />
You want the data and metadata to be linked with each other as tightly as possible. In fact, ideally you want them to be part of the same file and even, of the same type.</p>

<p>A critical aspect of any data format is maintaining the link between data and metadata throughout all stages of analysis. As data is processed, it often undergoes changes that require processing of the corresponding metadata.</p>
<ul>
  <li>Reduction: Data often gets reduced through operations like averaging. In the reduced data, certain metadata must be discarded while others retained. For example, if I average an animal’s speed across trials, the trial-level metadata is no longer relevant, but the animal ID must be preserved.</li>
  <li>Combination: when combining data from two or more sources, their respective metadata must be merged appropriately to maintain context and traceability.</li>
</ul>

<h2 id="data-formats">Data formats</h2>
<p>To reiterate, when choosing a data format, we want it to possess three features:</p>
<ol>
  <li>Easy and intuitive retrieval of data</li>
  <li>Extension to arbitrary number of dimensions</li>
  <li>Linkage between data and metadata to be maintained across analysis</li>
</ol>

<p><a href="https://numpy.org/">Numpy</a> arrays: Think of numpy arrays as a huge collection of numbers that are arranged in an n-dimensional cuboid.
<a href="https://pandas.pydata.org/">Pandas</a> dataframe: This is the most powerful and widely used format for storing data, and also the easiest to visualize. If you’ve ever seen a table in Excel or any data analysis software, you’ve seen a DataFrame. One major advantage of pandas DataFrames is their ubiquity in the Python ecosystem. They integrate seamlessly with other libraries, such as Seaborn and Plotly - two powerful tools for data visualization.</p>

<!-- ```python
df.loc[[1,2], 'col1']
array[i,j]
``` -->

<p>In NumPy arrays, each data point is accessed using a list of indices. Since these indices are just arbitrary numbers, it’s difficult to associate them with physically meaningful features.
DataFrames offer more flexibility in this regard: columns can be labeled with names, and rows can have meaningful indices (either numbers, as in serial IDs or strings). Additionally, filtering allows you to easily select specific subsets of rows.
This presents a classic tradeoff: NumPy arrays support arbitrary dimensions but data retrieval is difficult due to numerical indices, while DataFrames offer easier retrieval capabilities but are restricted to just two dimensions.
The ideal dataformat would be one that combines these two features. This is where <a href="https://xarray.dev/">xarray</a> comes in. Developed by data scientists working with climate data, xarray is basically an n-dimensional dataframe. The best way to get started with xarray is of course <a href="https://tutorial.xarray.dev/intro.html">the tutorial</a> prepared and maintained by creators.</p>

<p>Xarray has 3 components:</p>
<ul>
  <li>The data, shapes like an n-dimensional cuboid</li>
  <li>Each dimension has a name. In our behavioural dataset, the dimensions would be <strong>Position</strong>, <strong>Animals</strong>, <strong>Sessions</strong> and <strong>Stimuli</strong></li>
  <li>Coordinates: The values along dimensions are called coordinates. 
dim: Timel coords: 0, 1, 2, 3…. 
dim: Position; coords: x, y
dim: Animals; coords: animal1, animal2, animal3……
dim: Sessions, coords: 1, 2, 3….<br />
Together, these components make Xarray a powerful way to work with structured multi-dimensional data. By naming dimensions and assigning meaningful coordinates, you can index, slice, and align data intuitively without having to remember raw array indices. This makes analysis more readable and less error-prone.</li>
</ul>

<p>Ah! I see so xarrays are the way to go? Well, not quite. It took me some time to realise that xarrays weren’t the answer to all my prayers after all. In my case, the dimensionality was low, the number of unique stimulus features was small enough that a pandas DataFrame was sufficient.</p>]]></content><author><name>rsatapat</name><email>rsatapat@yahoo.com</email></author><category term="PhD" /><category term="Data-analysis" /><summary type="html"><![CDATA[For a data analyst, a large datasets can be a double-edge sword. When used properly, they can yield powerful insights and leave you with a deeper understaning of the world around you. However, in the absence of good data organisation skills, you are only left with wasted effort and confusion. I encountered this challenge firsthand during my PhD.]]></summary></entry><entry><title type="html">On behavioural neurocience and its assumptions</title><link href="http://localhost:4000/blogs/Phd-blog/" rel="alternate" type="text/html" title="On behavioural neurocience and its assumptions" /><published>2025-03-31T00:00:00+02:00</published><updated>2025-03-31T00:00:00+02:00</updated><id>http://localhost:4000/blogs/Phd-blog</id><content type="html" xml:base="http://localhost:4000/blogs/Phd-blog/"><![CDATA[<h2 id="overview">Overview</h2>
<h3 id="the-background">The background</h3>
<p>For my PhD, I studied how the invertebrate visual system guides locomotion. In simpler terms, I studied how fruit flies see and walk (yes, I’m aware of the irony)? As the title of my thesis suggests, I focused on how the fly’s brain assimilates visual stimuli (via integration or competition) to generate appropriate movement. Though I used fruit flies as a model, the underlying problem: how animals reconcile conflicting sensory inputs, is universal.<br />
For example, seeing and hearing a dog must be combined into a unified perception.</p>

<p>For example, seeing and hearing a dog must be combined into a unified perception. Think about seeing a dog and hearing a dog bark; there has to be a mechanism such that these two stimuli can be integrated in order to a unified consistent perception of a dog. On the other hand, think of a situation 
[image]
My PhD consisted of two primary projects that address these two complimentary aspects of perception:</p>
<ol>
  <li>Integration: How are visual stimuli from the two halveds of the visual field summed in the brain of the fly?</li>
  <li>Competition: How does a fly, engaged in a visual task, respond when shown another distracting visual stimulus?</li>
</ol>

<h4 id="animal-as-a-blackbox-and-tools-of-a-neuroscientist">animal as a blackbox and tools of a neuroscientist</h4>
<p>Lets first set the ground rules. For this project, we will imagine an animal to be a black box that receives some stimulus (the input) from its environment (the world) and produces a behaviour (the output). There are many reasons to reject this simplistic model, one of the most obvious ones being that an animal does not need any external input to produce behaviour, it is capable of doing that on its own. Tinbergen would probably frame it to say the stimulus isn’t the impetus for behaviour (the impetus already resides within the animal) but, merely acts as a trigger to release the impetus. That is true and we can get back to this discussion at the end, or maybe in a seprate post, but for now lets stick with our blackbox model.</p>

<p>On top of this, we have techniques, lets use the umbrella term “prodding”, that allow us to get some information regarding the internal workings of the blackbox.</p>

<p>For my PhD, the blackbox is a fruitfly, specifically the visual system of the fruit fly and the behaviour is locomotion, specifically walking. Then we have our tools for prodding, electrophysiology and microscopy we can get a glimpse of the internal of this blackbox. Additionally, using genetics we can causes perturbations in the blackbox that cause it to behave differently.</p>

<p>The game is to intelligently use stimuli, genetic perturbations and electrophysiology to understand what it is that the blackbody is doing.</p>

<h3 id="more-about-visual-integration">More about visual integration</h3>
<p>To be more precise, we wanted to understand how a fly is able to integrate stimuli in order to generate appropriate locomotion. The word “integrate” is doing a lot of heavy lifting here, so, let’s dig into that.<br />
What do I mean by integrating stimuli and why is it a problem?
There are three aspects to this issue:</p>
<ol>
  <li>Animals often multiple sensory modalities</li>
  <li>Any one sensory modality can parse many aspects of the stimuli, for example, eyes can gather information regarding</li>
  <li>There are too many things in this world</li>
</ol>]]></content><author><name>rsatapat</name><email>rsatapat@yahoo.com</email></author><category term="PhD" /><summary type="html"><![CDATA[For my PhD, I studied how the invertebrate visual system guides locomotion. In simpler terms, I studied how fruit flies see and walk (yes, I’m aware of the irony)? As the title of my thesis suggests, I focused on how the fly’s brain assimilates visual stimuli (via integration or competition) to generate appropriate movement. Though I used fruit flies as a model, the underlying problem-how animals reconcile conflicting sensory inputs, is universal.]]></summary></entry><entry><title type="html">On closedloop visual experiments</title><link href="http://localhost:4000/blogs/closedloop-experiment/" rel="alternate" type="text/html" title="On closedloop visual experiments" /><published>2025-03-31T00:00:00+02:00</published><updated>2025-03-31T00:00:00+02:00</updated><id>http://localhost:4000/blogs/closedloop-experiment</id><content type="html" xml:base="http://localhost:4000/blogs/closedloop-experiment/"><![CDATA[<p>To study the visual system, I built a closed-loop behavioral setup that allowed us to present visual stimuli to a walking fruit fly and update the stimulus in real time based on the fly’s position and orientation. Stripping away the scientific context, the goal was essentially to display a pinwheel disc to the fly such that:</p>
<ul>
  <li>it always remains centered on the fly, and</li>
  <li>it rotates in sync with the fly’s turns.</li>
</ul>

<p>Below is a pseudocode outline of how these closed-loop experiments were conducted.</p>
<pre><code class="language-pseudocode">define total_trial_num
define frames_per_trial

while trial&lt;frames_per_trial
    while frame&lt;frame_num
        grab image from camera
        extract location (loc) and orientaion (ori) of fly
        define parameters of stimulus using loc and ori
        send stimulus parameters to project
        projector shows updated stimulus
        save loc and ori to a csv file
        save image to a video
        frame=frame+1
    trial=trial+1
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">while </span><span class="p">(</span><span class="n">trials</span> <span class="o">&lt;</span> <span class="n">num_trials</span><span class="p">):</span><span class="c1"># add comment
</span>    <span class="nf">while</span><span class="p">(</span><span class="bp">True</span><span class="p">):</span><span class="c1"># add comment
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">ori</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">get_fly_postion_and_orientaion</span><span class="p">()</span><span class="c1"># add comment
</span>        <span class="n">stimulus</span><span class="p">.</span><span class="n">pos</span> <span class="o">=</span> <span class="n">loc</span><span class="c1"># add comment
</span>        <span class="n">stimulus</span><span class="p">.</span><span class="n">ori</span> <span class="o">=</span> <span class="n">ori</span><span class="c1"># add comment
</span>        <span class="n">stimulus</span><span class="p">.</span><span class="nf">draw</span><span class="p">()</span> <span class="c1"># add comment
</span>        <span class="n">win</span><span class="p">.</span><span class="nf">flip</span><span class="p">()</span> <span class="c1"># add comment
</span>
        <span class="k">if</span> <span class="n">frame</span><span class="o">==</span><span class="n">trial_length</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">frame</span><span class="o">+=</span><span class="mi">1</span>
    <span class="n">triala</span><span class="o">+=</span><span class="mi">1</span>
</code></pre></div></div>

<p>There are two key aspects of this pseudocode that deserve a deeper look:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>extract location (loc) and orientaion (ori) of fly
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>send stimulus parameters to project
projector shows updated stimulus
</code></pre></div></div>
<h2 id="how-to-extract-a-fly">How to extract a fly</h2>
<p>Extracting the location and orientation of the fly is surprisingly easy with OpenCV.
First, we use <code class="language-plaintext highlighter-rouge">cv2.threshold(cv_image, 60, 255, cv2.THRESH_BINARY)</code> to binarize the image—pixels darker than 60 become black (0), and brighter pixels become white (255). The threshold value of 60 is hardcoded here, but can also be determined automatically using Otsu’s method.</p>

<p>Next, we extract contours of all closed shapes in the image and apply an area filter to discard shapes that are too large or too small to be the fly. These area bounds are also hardcoded. Ideally, this step leaves us with a single contour that matches our criteria.</p>

<p>Finally, we use <code class="language-plaintext highlighter-rouge">cv2.fitEllipse</code> to fit an ellipse to the selected contour. The centroid (<code class="language-plaintext highlighter-rouge">cx = int(M['m10'] / M['m00'])</code>, <code class="language-plaintext highlighter-rouge">cy = int(M['m01'] / M['m00'])</code>) and the orientation of the ellipse’s major axis give us the fly’s position and orientation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_fly_postion_and_orientaion</span><span class="p">(</span><span class="n">camera</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">timestamp</span> <span class="o">=</span> <span class="n">image_methods</span><span class="p">.</span><span class="nf">grab_image</span><span class="p">(</span><span class="n">camera</span><span class="p">,</span> <span class="sh">'</span><span class="s">ptgrey</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># ptgrey is class/type of camera
</span>    <span class="n">cv_image</span> <span class="o">=</span> <span class="nc">ROI</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="c1"># cut out a section of the image, can be ignored
</span>    <span class="n">ret</span><span class="p">,</span> <span class="n">diff_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">threshold</span><span class="p">(</span><span class="n">cv_image</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span> <span class="c1"># binarise image, only the 
</span>    <span class="n">this</span><span class="p">,</span> <span class="n">contours</span><span class="p">,</span> <span class="n">hierarchy</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">findContours</span><span class="p">(</span><span class="n">diff_img</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">RETR_LIST</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">CHAIN_APPROX_SIMPLE</span><span class="p">)</span> 
    <span class="n">pos</span><span class="p">,</span> <span class="n">ellipse</span><span class="p">,</span> <span class="n">cnt</span> <span class="o">=</span> <span class="nf">fly_court_pos</span><span class="p">(</span><span class="n">contours</span><span class="p">,</span> <span class="n">size_cutoff</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">)</span>
    <span class="n">fly_ori</span> <span class="o">=</span> <span class="n">ellipse</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">fly_ori</span><span class="p">,</span> <span class="n">timestamp</span>

<span class="k">def</span> <span class="nf">fly_court_pos</span><span class="p">(</span><span class="n">contours</span><span class="p">,</span><span class="n">size_cutoff</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">max_size</span> <span class="o">=</span> <span class="mi">99999999</span><span class="p">):</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ellipse</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cnt</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">contours</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">contours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">15</span> <span class="ow">and</span> <span class="nf">len</span><span class="p">(</span><span class="n">contours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
            <span class="n">M</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">moments</span><span class="p">(</span><span class="n">contours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">Area</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">contourArea</span><span class="p">(</span><span class="n">contours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">Area</span> <span class="o">&gt;</span> <span class="n">size_cutoff</span> <span class="ow">and</span> <span class="n">Area</span> <span class="o">&lt;</span> <span class="n">max_size</span><span class="p">:</span>
                <span class="n">cx</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="sh">'</span><span class="s">m10</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="n">M</span><span class="p">[</span><span class="sh">'</span><span class="s">m00</span><span class="sh">'</span><span class="p">])</span>
                <span class="n">cy</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="sh">'</span><span class="s">m01</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="n">M</span><span class="p">[</span><span class="sh">'</span><span class="s">m00</span><span class="sh">'</span><span class="p">])</span>
                <span class="n">pos</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">])</span>
                <span class="n">cnt</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">contours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">ellipse</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">fitEllipse</span><span class="p">(</span><span class="n">contours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pos</span><span class="p">,</span><span class="n">ellipse</span><span class="p">,</span><span class="n">cnt</span>
</code></pre></div></div>

<h2 id="how-to-make-a-fly-see-things">How to make a fly see things</h2>
<p>Displaying visual stimuli is fairly straightforward using the PsychoPy library. The process involves three main steps:</p>
<ul>
  <li><strong>Define the display window</strong>: This is the frame inside which the stimulus appears. <a href="https://www.psychopy.org/api/visual/window.html#psychopy.visual.Window">We can set its size and brightness</a>. In our case, the window exactly overlaps with the arena boundaries. This alignment is crucial since the fly’s position is extracted in pixel coordinates. It must map precisely onto the display window to ensure accurate stimulus placement. There’s some nuance here, which I’ll cover in a future blog post.</li>
  <li><strong>Define the stimulus</strong>: PsychoPy offers a <a href="https://www.psychopy.org/api/visual/">wide array of stimuli</a>. For my experiments, I used <a href="https://www.psychopy.org/api/visual/imagestim.html">ImageStim</a>, which allows displaying an image that can be moved and rotated as needed.</li>
  <li><strong>Update and present</strong>: On every frame, the stimulus is updated, drawn onto the projector buffer, and then the window is flipped (i.e., the buffer is pushed to the screen) to present the stimulus.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">psychopy.visual</span>

<span class="n">win</span> <span class="o">=</span> <span class="n">psychopy</span><span class="p">.</span><span class="n">visual</span><span class="p">.</span><span class="nc">Window</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span>
    <span class="n">monitor</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">fullscr</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">waitBlanking</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stimulus</span> <span class="o">=</span> <span class="n">psychopy</span><span class="p">.</span><span class="n">visual</span><span class="p">.</span><span class="nc">ImageStim</span><span class="p">(</span>
    <span class="n">win</span><span class="o">=</span><span class="n">win</span><span class="p">,</span>
    <span class="n">image</span><span class="o">=</span><span class="n">stimulus_image</span><span class="p">,</span> <span class="c1"># path to image
</span>    <span class="n">mask</span><span class="o">=</span><span class="sh">'</span><span class="s">circle</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># clip a circle out of the image
</span>    <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">ori</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># 0 is vertical,positive values are rotated clockwise
</span>    <span class="n">size</span><span class="o">=</span><span class="n">stim_size</span> <span class="c1"># goes from 0 to 2 by default
</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">stimulus</span><span class="p">.</span><span class="n">autoDraw</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div>
<h2 id="parallelism-to-the-rescue">Parallelism to the rescue</h2>
<p>While the above schema works, it has a major limitation: all steps (image capture, extraction of position and orientation, stimulus update, and projector refresh) run within the same loop. This means the entire process is constrained by the slowest step, which is the projector update, which is limited to 60 Hz, in our case. However, the camera can capture images at a much higher rate (upto 150Hz, in our case), and we can track the fly’s position and orientation fast enough to keep up.</p>

<p>Ideally, we want to decouple image acquisition from stimulus presentation by running them in parallel processes. This allows faster image capture without being bottlenecked by projector update. This is where Python’s multipoecessing module comes to our rescue.</p>
<pre><code class="language-pseudocode">imaging_process()
    while trial&lt;frames_per_trial
        while frame&lt;frame_num
            grab image from camera
            extract location (loc) and orientaion (ori) of fly
            put loc,ori data into queue1
            save loc and ori to a csv file
            save image to a video
            frame=frame+1
            put frame into queue2
        trial=trial+1

stimulus_presentation_process()
    while trial&lt;frames_per_trial
        while frame&lt;frame_num
            get loc,ori data from queue1
            define parameters of stimulus using loc and ori
            projector shows updated stimulus
            frame=read frame from queue2
        trial=trial+1

main()
    define multiprocessing manager
    start manager

    use manager to create LIFO (last in first out) queue1
    use manaeger to create LIFO (last in first out) queue2

    define stimulis process
    define imaging process

    start stimulus process
    start imaging process
</code></pre>]]></content><author><name>rsatapat</name><email>rsatapat@yahoo.com</email></author><category term="PhD" /><category term="Python" /><summary type="html"><![CDATA[For a data analyst, enormous amounts of data can be a double-edge sword. While this opens new doors for understanding the world, good data organisation skills are essential if we are to make useful insights. I encountered this challenge firsthand during my PhD.]]></summary></entry></feed>